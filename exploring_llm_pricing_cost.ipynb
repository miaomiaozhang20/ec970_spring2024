{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miaomiaozhang20/ec970_spring2024/blob/main/exploring_llm_pricing_cost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimentation setup"
      ],
      "metadata": {
        "id": "HM3pGWkRDvrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cost_per_token(cost_per_million_tokens):\n",
        "    cost_per_token = cost_per_million_tokens / 1_000_000\n",
        "    return cost_per_token"
      ],
      "metadata": {
        "id": "XQpkEeenVs8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = \"\"\"\n",
        "In your own words, explain strengths and weaknesses of Nvidia's business model, given the following context:\n",
        "\n",
        "```\n",
        "NVIDIA pioneered accelerated computing to help solve the most challenging computational problems. NVIDIA is now a full-stack computing infrastructure\n",
        "company with data-center-scale offerings that are reshaping industry.\n",
        "Our full-stack includes the foundational CUDA programming model that runs on all NVIDIA GPUs, as well as hundreds of domain-specific software libraries,\n",
        "software development kits, or SDKs, and Application Programming Interfaces, or APIs. This deep and broad software stack accelerates the performance and\n",
        "eases the deployment of NVIDIA accelerated computing for computationally intensive workloads such as artificial intelligence, or AI, model training and\n",
        "inference, data analytics, scientific computing, and 3D graphics, with vertical-specific optimizations to address industries ranging from healthcare and telecom to\n",
        "automotive and manufacturing.\n",
        "Our data-center-scale offerings are comprised of compute and networking solutions that can scale to tens of thousands of GPU-accelerated servers\n",
        "interconnected to function as a single giant computer; this type of data center architecture and scale is needed for the development and deployment of modern\n",
        "AI applications.\n",
        "The GPU was initially used to simulate human imagination, enabling the virtual worlds of video games and films. Today, it also simulates human intelligence,\n",
        "enabling a deeper understanding of the physical world. Its parallel processing capabilities, supported by thousands of computing cores, are essential for deep\n",
        "learning algorithms. This form of AI, in which software writes itself by learning from large amounts of data, can serve as the brain of computers, robots and self\n",
        "driving cars that can perceive and understand the world. GPU-powered AI solutions are being developed by thousands of enterprises to deliver services and\n",
        "products that would have been immensely difficult or even impossible with traditional coding. Examples include generative AI, which can create new content\n",
        "such as text, code, images, audio, video, and molecule structures, and recommendation systems, which can recommend highly relevant content such as\n",
        "products, services, media or ads using deep neural networks trained on vast datasets that capture the user preferences.\n",
        "NVIDIA has a platform strategy, bringing together hardware, systems, software, algorithms, libraries, and services to create unique value for the markets we\n",
        "serve. While the computing requirements of these end markets are diverse, we address them with a unified underlying architecture leveraging our GPUs and\n",
        "networking and software stacks. The programmable nature of our architecture allows us to support several multi-billion-dollar end markets with the same\n",
        "underlying technology by using a variety of software stacks developed either internally or by third-party developers and partners. The large and growing number\n",
        "of developers and installed base across our platforms strengthens our ecosystem and increases the value of our platform to our customers.\n",
        "Innovation is at our core. We have invested over $45.3 billion in research and development since our inception, yielding inventions that are essential to modern\n",
        "computing. Our invention of the GPU in 1999 sparked the growth of the PC gaming market and redefined computer graphics. With our introduction of the CUDA\n",
        "programming model in 2006, we opened the parallel processing capabilities of our GPU to a broad range of compute-intensive applications, paving the way for\n",
        "the emergence of modern AI. In 2012, the AlexNet neural network, trained on NVIDIA GPUs, won the ImageNet computer image recognition competition,\n",
        "marking the “Big Bang” moment of AI. We introduced our first Tensor Core GPU in 2017, built from the ground-up for the new era of AI, and our first autonomous\n",
        "driving system-on-chips, or SoC, in 2018. Our acquisition of Mellanox in 2020 expanded our innovation canvas to include networking and led to the introduction\n",
        "of a new processor class – the data processing unit, or DPU. Over the past 5 years, we have built full software stacks that run on top of our GPUs and CUDA to\n",
        "bring AI to the world’s largest industries, including NVIDIA DRIVE stack for autonomous driving, Clara for healthcare, and Omniverse for industrial digitalization;\n",
        "and introduced the NVIDIA AI Enterprise software – essentially an operating system for enterprise AI applications. In 2023, we introduced our first data center\n",
        "CPU, Grace, built for giant-scale AI and high-performance computing. With a strong engineering culture, we drive fast, yet harmonized, product and technology\n",
        "innovations in all dimensions of computing including silicon, systems, networking, software and algorithms. More than half of our engineers work on software.\n",
        "The world’s leading cloud service providers, or CSPs, and consumer internet companies use our data center-scale accelerated computing platforms to enable,\n",
        "accelerate or enrich the services they deliver to billions of end users, including AI solutions and assistants, search, recommendations, social networking, online\n",
        "shopping, live video, and translation.\n",
        "Enterprises and startups across a broad range of industries use our accelerated computing platforms to build new generative AI-enabled products and services,\n",
        "or to dramatically accelerate and reduce the costs of their workloads and workflows. The enterprise software industry uses them for new AI assistants and\n",
        "chatbots; the transportation industry for autonomous driving; the healthcare industry for accelerated and computer-aided drug discovery; and the financial\n",
        "services industry for customer support and fraud detection.\n",
        "\n",
        "Researchers and developers use our computing solutions to accelerate a wide range of important applications, from simulating molecular dynamics to climate\n",
        "forecasting. With support for more than 3,500 applications, NVIDIA computing enables some of the most promising areas of discovery, from climate prediction to\n",
        "materials science and from wind tunnel simulation to genomics. Including GPUs and networking, NVIDIA powers over 75% of the supercomputers on the global\n",
        "TOP500 list, including 24 of the top 30 systems on the Green500 list.\n",
        "Gamers choose NVIDIA GPUs to enjoy immersive, increasingly cinematic virtual worlds. In addition to serving the growing number of gamers, the market for PC\n",
        "GPUs is expanding because of the burgeoning population of live streamers, broadcasters, artists, and creators. With the advent of generative AI, we expect a\n",
        "broader set of PC users to choose NVIDIA GPUs for running generative AI applications locally on their PC, which is critical for privacy, latency, and cost-sensitive\n",
        "AI applications.\n",
        "Professional artists, architects and designers use NVIDIA partner products accelerated with our GPUs and software platform for a range of creative and design\n",
        "use cases, such as creating visual effects in movies or designing buildings and products. In addition, generative AI is expanding the market for our workstation\n",
        "class GPUs, as more enterprise customers develop and deploy AI applications with their data on-premises.\n",
        "Headquartered in Santa Clara, California, NVIDIA was incorporated in California in April 1993 and reincorporated in Delaware in April 1998.\n",
        "```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Kv7qqQ-sFCB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Groq"
      ],
      "metadata": {
        "id": "jVZXLIXkCSK5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GuSmv7tA4Q3"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# Set your Groq API key\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "-rc4OIWRA-gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "def call_groq(query: str, model_name: str):\n",
        "  return client.chat.completions.create(\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": message,\n",
        "          }\n",
        "      ],\n",
        "      max_tokens=1000,\n",
        "      temperature=0.0,\n",
        "      model=model_name,\n",
        "  )"
      ],
      "metadata": {
        "id": "bClwZYUWBImn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define cost map\n",
        "model_costs = {\n",
        "    \"mixtral-8x7b-32768\": {\n",
        "        \"input_tokens\": 0.27,\n",
        "        \"output_tokens\": 0.27,\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "sgGOoaR6egea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, costs in model_costs.items():\n",
        "    num_iterations = 10\n",
        "    total_time = 0\n",
        "    total_output_tokens = 0\n",
        "    total_cost = 0\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Call the model\n",
        "        start = time.time()\n",
        "        response = call_groq(message, model_name=model_name)\n",
        "        end = time.time()\n",
        "\n",
        "        # Calculate costs\n",
        "        input_cost = calculate_cost_per_token(costs[\"input_tokens\"])\n",
        "        output_cost = calculate_cost_per_token(costs[\"output_tokens\"])\n",
        "        input_tokens = response.usage.prompt_tokens\n",
        "        output_tokens = response.usage.completion_tokens\n",
        "        iteration_cost = (input_tokens * input_cost) + (output_tokens * output_cost)\n",
        "\n",
        "        # Calculate time\n",
        "        iteration_time = end - start\n",
        "\n",
        "        # Calculate totals\n",
        "        total_time += iteration_time\n",
        "        total_output_tokens += output_tokens\n",
        "        total_cost += iteration_cost\n",
        "\n",
        "        # Prevent rate limiting\n",
        "        time.sleep(1)\n",
        "\n",
        "    avg_tokens_per_second = total_output_tokens / total_time\n",
        "    avg_cost = total_cost / num_iterations\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Average tokens per second: {avg_tokens_per_second:.2f}\")\n",
        "    print(f\"Average total cost: ${avg_cost:.5f}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "ll1wBu1Fesj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anthropic"
      ],
      "metadata": {
        "id": "i59xmua5CT_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q anthropic"
      ],
      "metadata": {
        "id": "O7NGayb1FUmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "TPiFKSEdFaqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define code to call model"
      ],
      "metadata": {
        "id": "0SSh_iWSZNaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from anthropic import Anthropic\n",
        "\n",
        "client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\n",
        "\n",
        "def call_anthropic(query: str, model_name: str) -> str:\n",
        "  return client.messages.create(\n",
        "      temperature=0.0,\n",
        "      max_tokens=1000,\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": message,\n",
        "          },\n",
        "      ],\n",
        "      model=model_name,\n",
        "  )"
      ],
      "metadata": {
        "id": "vpqQ254xCVQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate cost and speed"
      ],
      "metadata": {
        "id": "S1_IYnXLVzib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define cost map\n",
        "model_costs = {\n",
        "    \"claude-3-haiku-20240307\": {\n",
        "        \"input_tokens\": 0.25,\n",
        "        \"output_tokens\": 1.25,\n",
        "    },\n",
        "    \"claude-3-sonnet-20240229\": {\n",
        "        \"input_tokens\": 3.00,\n",
        "        \"output_tokens\": 15.00,\n",
        "    },\n",
        "    \"claude-3-opus-20240229\": {\n",
        "        \"input_tokens\": 15.00,\n",
        "        \"output_tokens\": 75.00,\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "pWkU3K_vX04J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, costs in model_costs.items():\n",
        "    num_iterations = 10\n",
        "    total_time = 0\n",
        "    total_output_tokens = 0\n",
        "    total_cost = 0\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Call the model\n",
        "        start = time.time()\n",
        "        response = call_anthropic(message, model_name=model_name)\n",
        "        end = time.time()\n",
        "\n",
        "        # Calculate costs\n",
        "        input_cost = calculate_cost_per_token(costs[\"input_tokens\"])\n",
        "        output_cost = calculate_cost_per_token(costs[\"output_tokens\"])\n",
        "        input_tokens = response.usage.input_tokens\n",
        "        output_tokens = response.usage.output_tokens\n",
        "        iteration_cost = (input_tokens * input_cost) + (output_tokens * output_cost)\n",
        "\n",
        "        # Calculate time\n",
        "        iteration_time = end - start\n",
        "\n",
        "        # Calculate totals\n",
        "        total_time += iteration_time\n",
        "        total_output_tokens += output_tokens\n",
        "        total_cost += iteration_cost\n",
        "\n",
        "        # Prevent rate limiting\n",
        "        time.sleep(1)\n",
        "\n",
        "    avg_tokens_per_second = total_output_tokens / total_time\n",
        "    avg_cost = total_cost / num_iterations\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Average tokens per second: {avg_tokens_per_second:.2f}\")\n",
        "    print(f\"Average total cost: ${avg_cost:.5f}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "BWx21JRaFxLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cohere"
      ],
      "metadata": {
        "id": "45bTQ0QECWKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q cohere"
      ],
      "metadata": {
        "id": "zg0pCsqdCW83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your Cohere API key\n",
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "OoY0cV2JZZ6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "\n",
        "# Get your cohere API key on: www.cohere.com\n",
        "co = cohere.Client(os.environ[\"COHERE_API_KEY\"])\n",
        "\n",
        "def call_cohere(query: str, model_name: str) -> str:\n",
        "  return co.chat(\n",
        "      message=query,\n",
        "      max_tokens=1000,\n",
        "      temperature=0.0,\n",
        "  )"
      ],
      "metadata": {
        "id": "dmOlMgkvZeSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define cost map\n",
        "model_costs = {\n",
        "    \"command-light\": {\n",
        "        \"input_tokens\": 0.30,\n",
        "        \"output_tokens\": 0.60,\n",
        "    },\n",
        "    \"command-r\": {\n",
        "        \"input_tokens\": 0.50,\n",
        "        \"output_tokens\": 1.50,\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "cc0P1kBecSSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, costs in model_costs.items():\n",
        "    num_iterations = 10\n",
        "    total_time = 0\n",
        "    total_output_tokens = 0\n",
        "    total_cost = 0\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Call the model\n",
        "        start = time.time()\n",
        "        response = call_cohere(message, model_name=model_name)\n",
        "        end = time.time()\n",
        "\n",
        "        # Calculate costs\n",
        "        input_cost = calculate_cost_per_token(costs[\"input_tokens\"])\n",
        "        output_cost = calculate_cost_per_token(costs[\"output_tokens\"])\n",
        "        input_tokens = response.meta[\"billed_units\"][\"input_tokens\"]\n",
        "        output_tokens = response.meta[\"billed_units\"][\"output_tokens\"]\n",
        "        iteration_cost = (input_tokens * input_cost) + (output_tokens * output_cost)\n",
        "\n",
        "        # Calculate time\n",
        "        iteration_time = end - start\n",
        "\n",
        "        # Calculate totals\n",
        "        total_time += iteration_time\n",
        "        total_output_tokens += output_tokens\n",
        "        total_cost += iteration_cost\n",
        "\n",
        "        # Prevent rate limiting\n",
        "        time.sleep(1)\n",
        "\n",
        "    avg_tokens_per_second = total_output_tokens / total_time\n",
        "    avg_cost = total_cost / num_iterations\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Average tokens per second: {avg_tokens_per_second:.2f}\")\n",
        "    print(f\"Average total cost: ${avg_cost:.5f}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "O-CIPDXAcQ3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mistral"
      ],
      "metadata": {
        "id": "vmfcSgY0CYGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q mistralai"
      ],
      "metadata": {
        "id": "X0ZlhHwUgkuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your Mistral API key\n",
        "os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "LRY4UnH1ZZDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "\n",
        "client = MistralClient(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
        "\n",
        "def call_mistral(query: str, model_name: str) -> str:\n",
        "  return client.chat(\n",
        "      model=model_name,\n",
        "      max_tokens=1000,\n",
        "      temperature=0.0,\n",
        "      messages=[\n",
        "        ChatMessage(role=\"user\", content=query)\n",
        "      ]\n",
        "  )"
      ],
      "metadata": {
        "id": "hfxXZaE4CZnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define cost map\n",
        "model_costs = {\n",
        "    \"mistral-small-2312\": {\n",
        "        \"input_tokens\": 0.70,\n",
        "        \"output_tokens\": 0.70,\n",
        "    },\n",
        "    \"mistral-large-2402\": {\n",
        "        \"input_tokens\": 8.00,\n",
        "        \"output_tokens\": 24.00,\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "v2WVr_XjhCkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, costs in model_costs.items():\n",
        "    num_iterations = 10\n",
        "    total_time = 0\n",
        "    total_tokens_per_second = 0\n",
        "    total_cost = 0\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Call the model\n",
        "        start = time.time()\n",
        "        response = call_mistral(message, model_name=model_name)\n",
        "        end = time.time()\n",
        "\n",
        "        # Calculate costs\n",
        "        input_cost = calculate_cost_per_token(costs[\"input_tokens\"])\n",
        "        output_cost = calculate_cost_per_token(costs[\"output_tokens\"])\n",
        "        input_tokens = response.usage.prompt_tokens\n",
        "        output_tokens = response.usage.completion_tokens\n",
        "        iteration_cost = (input_tokens * input_cost) + (output_tokens * output_cost)\n",
        "\n",
        "        # Calculate time\n",
        "        iteration_time = end - start\n",
        "\n",
        "        # Calculate totals\n",
        "        total_time += iteration_time\n",
        "        total_output_tokens += output_tokens\n",
        "        total_cost += iteration_cost\n",
        "\n",
        "        # Prevent rate limiting\n",
        "        time.sleep(1)\n",
        "\n",
        "    avg_tokens_per_second = total_output_tokens / total_time\n",
        "    avg_cost = total_cost / num_iterations\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Average tokens per second: {avg_tokens_per_second:.2f}\")\n",
        "    print(f\"Average total cost: ${avg_cost:.5f}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "bPBEhXPzhQnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI"
      ],
      "metadata": {
        "id": "7rYyonhICaKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q openai"
      ],
      "metadata": {
        "id": "IhrEiso7CawL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "--ezz5Dvh7jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "def call_openai(query: str, model_name: str) -> str:\n",
        "  return client.chat.completions.create(\n",
        "      model=model_name,\n",
        "      temperature=0,\n",
        "      max_tokens=1000,\n",
        "      messages=[\n",
        "          {\"role\": \"user\", \"content\": query},\n",
        "      ]\n",
        "  )"
      ],
      "metadata": {
        "id": "UyBwlTUzhpdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define cost map\n",
        "model_costs = {\n",
        "    \"gpt-3.5-turbo-0125\": {\n",
        "        \"input_tokens\": 0.50,\n",
        "        \"output_tokens\": 1.50,\n",
        "    },\n",
        "    \"gpt-4-0125-preview\": {\n",
        "        \"input_tokens\": 10.00,\n",
        "        \"output_tokens\": 30.00,\n",
        "    },\n",
        "}"
      ],
      "metadata": {
        "id": "itrUZYEeioy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, costs in model_costs.items():\n",
        "    num_iterations = 10\n",
        "    total_time = 0\n",
        "    total_output_tokens = 0\n",
        "    total_cost = 0\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Call the model\n",
        "        start = time.time()\n",
        "        response = call_openai(message, model_name=model_name)\n",
        "        end = time.time()\n",
        "\n",
        "        # Calculate costs\n",
        "        input_cost = calculate_cost_per_token(costs[\"input_tokens\"])\n",
        "        output_cost = calculate_cost_per_token(costs[\"output_tokens\"])\n",
        "        input_tokens = response.usage.prompt_tokens\n",
        "        output_tokens = response.usage.completion_tokens\n",
        "        iteration_cost = (input_tokens * input_cost) + (output_tokens * output_cost)\n",
        "\n",
        "        # Calculate time\n",
        "        iteration_time = end - start\n",
        "\n",
        "        # Calculate totals\n",
        "        total_time += iteration_time\n",
        "        total_output_tokens += output_tokens\n",
        "        total_cost += iteration_cost\n",
        "\n",
        "        # Prevent rate limiting\n",
        "        time.sleep(1)\n",
        "\n",
        "    avg_tokens_per_second = total_output_tokens / total_time\n",
        "    avg_cost = total_cost / num_iterations\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Average tokens per second: {avg_tokens_per_second:.2f}\")\n",
        "    print(f\"Average total cost: ${avg_cost:.5f}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "c4qQHBnLimLl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}